{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMYdNixsxKC0",
        "outputId": "2db618f2-289a-4168-d829-d672145fc527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# --- Step 1: Install dependencies ---\n",
        "!pip install ultralytics --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NevNG2YEwf0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48596a4e-8449-4933-cb96-9a07495e3ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "import os, cv2, numpy as np, contextlib, io\n",
        "from ultralytics import YOLO\n",
        "import glob\n",
        "\n",
        "\n",
        "os.environ[\"YOLO_VERBOSE\"] = \"False\"\n",
        "os.environ[\"KMP_WARNINGS\"] = \"0\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "class ObjectDetector:\n",
        "    def __init__(self, model_name='yolov8m.pt'):\n",
        "        self.model = YOLO(model_name)\n",
        "\n",
        "    def detect_objects(self, image_path, rows=2, cols=2, conf_threshold=0.45, imgsz=1280):\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            return None, None\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        tile_h, tile_w = h // rows, w // cols\n",
        "        detections_total, crops = [], []\n",
        "\n",
        "        for i in range(rows):\n",
        "            for j in range(cols):\n",
        "                y1, y2 = i * tile_h, (i + 1) * tile_h\n",
        "                x1, x2 = j * tile_w, (j + 1) * tile_w\n",
        "                crops.append(image[y1:y2, x1:x2])\n",
        "\n",
        "        # Suppress all YOLO output\n",
        "        with contextlib.redirect_stdout(io.StringIO()):\n",
        "            results = self.model.predict(crops, conf=conf_threshold, imgsz=imgsz, verbose=False)\n",
        "\n",
        "        for res in results:\n",
        "            for box in res.boxes:\n",
        "                detections_total.append(int(box.cls))\n",
        "\n",
        "        num_people = detections_total.count(0)\n",
        "        num_cars = sum(1 for cls in detections_total if cls in [2, 5, 7])  # car, bus, truck\n",
        "\n",
        "\n",
        "        return num_people, num_cars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr6IsGbxGtyF"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "detector = ObjectDetector()\n",
        "people, vehicle = detector.detect_objects(\"/content/traffic.jpg\")\n",
        "print(f\"ðŸ§ People detected: {people}\")\n",
        "print(f\"ðŸš— Vehicles detected: {vehicle}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqbryyNrFF_5",
        "outputId": "c89b542e-5671-4688-8680-c4cff20353cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/farzadnekouei/top-view-vehicle-detection-image-dataset/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"farzadnekouei/top-view-vehicle-detection-image-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/kagglehub/datasets/farzadnekouei/top-view-vehicle-detection-image-dataset/versions/2/Vehicle_Detection_Image_Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOkD7osFcinQ",
        "outputId": "2e889c79-bb56-4b51-bbf9-f597e5e474cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.yaml\t    README.roboflow.txt  sample_video.mp4  valid\n",
            "README.dataset.txt  sample_image.jpg\t train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "base = \"/root/.cache/kagglehub/datasets/farzadnekouei/top-view-vehicle-detection-image-dataset/versions/2/Vehicle_Detection_Image_Dataset\"\n",
        "# Load pretrained YOLOv8m model\n",
        "model = YOLO(\"yolov8m.pt\")\n",
        "\n",
        "# Train on your dataset\n",
        "model.train(\n",
        "    data=base+\"/data.yaml\",\n",
        "    epochs=40,          # can increase to 60â€“100 if needed\n",
        "    imgsz=960,\n",
        "    batch=16,\n",
        "    lr0=0.001,\n",
        "    pretrained=True,\n",
        "    name=\"yolov8m_vehicle_finetuned\",\n",
        "    project=\"Vehicle_Training\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU9L9IiBbOdG",
        "outputId": "c7796d7d-3c58-49c6-a60a-27f86e26ddaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.223 ðŸš€ Python-3.12.12 torch-2.8.0+cu126 CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/root/.cache/kagglehub/datasets/farzadnekouei/top-view-vehicle-detection-image-dataset/versions/2/Vehicle_Detection_Image_Dataset/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=40, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=960, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8m_vehicle_finetuned6, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=Vehicle_Training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/Vehicle_Training/yolov8m_vehicle_finetuned6, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.head.Detect           [1, [192, 384, 576]]          \n",
            "Model summary: 169 layers, 25,856,899 parameters, 25,856,883 gradients, 79.1 GFLOPs\n",
            "\n",
            "Transferred 469/475 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 21.9Â±3.9 MB/s, size: 57.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /root/.cache/kagglehub/datasets/farzadnekouei/top-view-vehicle-detection-image-dataset/versions/2/Vehicle_Detection_Image_Dataset/train/labels.cache... 536 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 536/536 554.4Kit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 22.6Â±4.5 MB/s, size: 58.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /root/.cache/kagglehub/datasets/farzadnekouei/top-view-vehicle-detection-image-dataset/versions/2/Vehicle_Detection_Image_Dataset/valid/labels.cache... 90 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 90/90 95.9Kit/s 0.0s\n",
            "Plotting labels to /content/Vehicle_Training/yolov8m_vehicle_finetuned6/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
            "Image sizes 960 train, 960 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/content/Vehicle_Training/yolov8m_vehicle_finetuned6\u001b[0m\n",
            "Starting training for 40 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bnj2-cJcxjK3",
        "outputId": "7145bc1e-76dc-42ef-9237-d4067d6e76a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 49.7MB 176.4MB/s 0.3s\n",
            "1/90 processed\n",
            "2/90 processed\n",
            "3/90 processed\n",
            "4/90 processed\n",
            "5/90 processed\n",
            "6/90 processed\n",
            "7/90 processed\n",
            "8/90 processed\n",
            "9/90 processed\n",
            "10/90 processed\n",
            "11/90 processed\n",
            "12/90 processed\n",
            "13/90 processed\n",
            "14/90 processed\n",
            "15/90 processed\n",
            "16/90 processed\n",
            "17/90 processed\n",
            "18/90 processed\n",
            "19/90 processed\n",
            "20/90 processed\n",
            "21/90 processed\n",
            "22/90 processed\n",
            "23/90 processed\n",
            "24/90 processed\n",
            "25/90 processed\n",
            "26/90 processed\n",
            "27/90 processed\n",
            "28/90 processed\n",
            "29/90 processed\n",
            "30/90 processed\n",
            "31/90 processed\n",
            "32/90 processed\n",
            "33/90 processed\n",
            "34/90 processed\n",
            "35/90 processed\n",
            "36/90 processed\n",
            "37/90 processed\n",
            "38/90 processed\n",
            "39/90 processed\n",
            "40/90 processed\n",
            "41/90 processed\n",
            "42/90 processed\n",
            "43/90 processed\n",
            "44/90 processed\n",
            "45/90 processed\n",
            "46/90 processed\n",
            "47/90 processed\n",
            "48/90 processed\n",
            "49/90 processed\n",
            "50/90 processed\n",
            "51/90 processed\n",
            "52/90 processed\n",
            "53/90 processed\n",
            "54/90 processed\n",
            "55/90 processed\n",
            "56/90 processed\n",
            "57/90 processed\n",
            "58/90 processed\n",
            "59/90 processed\n",
            "60/90 processed\n",
            "61/90 processed\n",
            "62/90 processed\n",
            "63/90 processed\n",
            "64/90 processed\n",
            "65/90 processed\n",
            "66/90 processed\n",
            "67/90 processed\n",
            "68/90 processed\n",
            "69/90 processed\n",
            "70/90 processed\n",
            "71/90 processed\n",
            "72/90 processed\n",
            "73/90 processed\n",
            "74/90 processed\n",
            "75/90 processed\n",
            "76/90 processed\n",
            "77/90 processed\n",
            "78/90 processed\n",
            "79/90 processed\n",
            "80/90 processed\n",
            "81/90 processed\n",
            "82/90 processed\n",
            "83/90 processed\n",
            "84/90 processed\n",
            "85/90 processed\n",
            "86/90 processed\n",
            "87/90 processed\n",
            "88/90 processed\n",
            "89/90 processed\n",
            "90/90 processed\n",
            "âœ… Tested on 90 images\n",
            "ðŸ§¾ Total True: 937, Predicted: 1125\n",
            "ðŸ“‰ Mean Absolute Error per image: 5.04\n",
            "ðŸŽ¯ Overall Accuracy: 0.80\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "# Assuming your ObjectDetector class is defined\n",
        "detector = ObjectDetector()\n",
        "\n",
        "# Paths\n",
        "base_path = \"/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset\"\n",
        "val_images = os.path.join(base_path, \"valid/images\")\n",
        "val_labels = os.path.join(base_path, \"valid/labels\")\n",
        "\n",
        "# Function to count vehicles from YOLO labels\n",
        "def count_true_vehicles(label_path):\n",
        "    if not os.path.exists(label_path):\n",
        "        return 0\n",
        "    with open(label_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    return len(lines)\n",
        "\n",
        "# --- Select random subset ---\n",
        "all_images = [f for f in os.listdir(val_images) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "# total_samples = 50\n",
        "# sample_images = random.sample(all_images, total_samples)\n",
        "total_samples = len(all_images)\n",
        "\n",
        "total_true, total_pred, errors = 0, 0, []\n",
        "count = 0\n",
        "for img_file in all_images:\n",
        "    img_path = os.path.join(val_images, img_file)\n",
        "    label_path = os.path.join(val_labels, img_file.rsplit('.', 1)[0] + \".txt\")\n",
        "\n",
        "    true_count = count_true_vehicles(label_path)\n",
        "    total_true += true_count\n",
        "\n",
        "    people, cars = detector.detect_objects(img_path)\n",
        "    pred_count = cars\n",
        "    total_pred += pred_count\n",
        "    errors.append(abs(pred_count - true_count))\n",
        "    count+=1\n",
        "    print(f\"{count}/{total_samples} processed\")\n",
        "\n",
        "mean_abs_error = sum(errors) / len(errors)\n",
        "accuracy = 1 - abs(total_pred - total_true) / max(total_true, 1)\n",
        "\n",
        "print(f\"âœ… Tested on {total_samples} images\")\n",
        "print(f\"ðŸ§¾ Total True: {total_true}, Predicted: {total_pred}\")\n",
        "print(f\"ðŸ“‰ Mean Absolute Error per image: {mean_abs_error:.2f}\")\n",
        "print(f\"ðŸŽ¯ Overall Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output:\n",
        "```shell\n",
        "âœ… Tested on 90 images\n",
        "ðŸ§¾ Total True: 937, Predicted: 1125\n",
        "ðŸ“‰ Mean Absolute Error per image: 5.04\n",
        "ðŸŽ¯ Overall Accuracy: 0.80\n",
        "```"
      ],
      "metadata": {
        "id": "G84tQZVq_YA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy testing for people"
      ],
      "metadata": {
        "id": "BnuIR_aFDXjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"fmena14/crowd-counting\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MiLXfhtDbS4",
        "outputId": "ea8e9c57-cda0-4a45-ac30-caf109b360ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/fmena14/crowd-counting/versions/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJmCTxu5MNDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d46122-9539-4d03-efee-ca14642c50d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2000 images and 2000 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-952515941.py:33: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  total_true += int(true_count)\n",
            "/tmp/ipython-input-952515941.py:35: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  errors.append(abs(int(people) - int(true_count)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/100 processed\n",
            "2/100 processed\n",
            "3/100 processed\n",
            "4/100 processed\n",
            "5/100 processed\n",
            "6/100 processed\n",
            "7/100 processed\n",
            "8/100 processed\n",
            "9/100 processed\n",
            "10/100 processed\n",
            "11/100 processed\n",
            "12/100 processed\n",
            "13/100 processed\n",
            "14/100 processed\n",
            "15/100 processed\n",
            "16/100 processed\n",
            "17/100 processed\n",
            "18/100 processed\n",
            "19/100 processed\n",
            "20/100 processed\n",
            "21/100 processed\n",
            "22/100 processed\n",
            "23/100 processed\n",
            "24/100 processed\n",
            "25/100 processed\n",
            "26/100 processed\n",
            "27/100 processed\n",
            "28/100 processed\n",
            "29/100 processed\n",
            "30/100 processed\n",
            "31/100 processed\n",
            "32/100 processed\n",
            "33/100 processed\n",
            "34/100 processed\n",
            "35/100 processed\n",
            "36/100 processed\n",
            "37/100 processed\n",
            "38/100 processed\n",
            "39/100 processed\n",
            "40/100 processed\n",
            "41/100 processed\n",
            "42/100 processed\n",
            "43/100 processed\n",
            "44/100 processed\n",
            "45/100 processed\n",
            "46/100 processed\n",
            "47/100 processed\n",
            "48/100 processed\n",
            "49/100 processed\n",
            "50/100 processed\n",
            "51/100 processed\n",
            "52/100 processed\n",
            "53/100 processed\n",
            "54/100 processed\n",
            "55/100 processed\n",
            "56/100 processed\n",
            "57/100 processed\n",
            "58/100 processed\n",
            "59/100 processed\n",
            "60/100 processed\n",
            "61/100 processed\n",
            "62/100 processed\n",
            "63/100 processed\n",
            "64/100 processed\n",
            "65/100 processed\n",
            "66/100 processed\n",
            "67/100 processed\n",
            "68/100 processed\n",
            "69/100 processed\n",
            "70/100 processed\n",
            "71/100 processed\n",
            "72/100 processed\n",
            "73/100 processed\n",
            "74/100 processed\n",
            "75/100 processed\n",
            "76/100 processed\n",
            "77/100 processed\n",
            "78/100 processed\n",
            "79/100 processed\n",
            "80/100 processed\n",
            "81/100 processed\n",
            "82/100 processed\n",
            "83/100 processed\n",
            "84/100 processed\n",
            "85/100 processed\n",
            "86/100 processed\n",
            "87/100 processed\n",
            "88/100 processed\n",
            "89/100 processed\n",
            "90/100 processed\n",
            "91/100 processed\n",
            "92/100 processed\n",
            "93/100 processed\n",
            "94/100 processed\n",
            "95/100 processed\n",
            "96/100 processed\n",
            "97/100 processed\n",
            "98/100 processed\n",
            "99/100 processed\n",
            "100/100 processed\n",
            "âœ… Tested on 100 random images\n",
            "ðŸ§¾ Total True: 3088, Predicted: 2504\n",
            "ðŸ“‰ Mean Absolute Error per image: 6.12\n",
            "ðŸŽ¯ Overall Accuracy: 0.81\n"
          ]
        }
      ],
      "source": [
        "# Your detector class\n",
        "detector = ObjectDetector()\n",
        "\n",
        "# Paths to crowd dataset (adjust accordingly)\n",
        "base = path\n",
        "model = ObjectDetector()\n",
        "# Load dataset\n",
        "images = np.load(base + \"/images.npy\")\n",
        "labels = np.load(base + \"/labels.npy\")\n",
        "\n",
        "# Ensure shapes match\n",
        "print(f\"Loaded {len(images)} images and {len(labels)} labels\")\n",
        "# Select a random subset of 100 frames\n",
        "COUNT = 100\n",
        "sample_indices = random.sample(range(len(images)), COUNT)\n",
        "sample_images = images[sample_indices]\n",
        "sample_labels = labels[sample_indices]\n",
        "\n",
        "total_true, total_pred, errors = 0, 0, []\n",
        "count = 0\n",
        "\n",
        "for img, true_count in zip(sample_images, sample_labels):\n",
        "    count += 1\n",
        "    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Save temporarily since detector expects a path\n",
        "    tmp_path = f\"/kaggle/temp_{count}.jpg\"\n",
        "    cv2.imwrite(tmp_path, img_bgr)\n",
        "\n",
        "    people, cars = detector.detect_objects(tmp_path, conf_threshold=0.25)\n",
        "    os.remove(tmp_path)\n",
        "\n",
        "    total_true += int(true_count)\n",
        "    total_pred += int(people)\n",
        "    errors.append(abs(int(people) - int(true_count)))\n",
        "\n",
        "    print(f\"{count}/{COUNT} processed\")\n",
        "\n",
        "mean_abs_error = sum(errors) / len(errors)\n",
        "accuracy = 1 - abs(total_pred - total_true) / max(total_true, 1)\n",
        "\n",
        "print(f\"âœ… Tested on {COUNT} random images\")\n",
        "print(f\"ðŸ§¾ Total True: {total_true}, Predicted: {total_pred}\")\n",
        "print(f\"ðŸ“‰ Mean Absolute Error per image: {mean_abs_error:.2f}\")\n",
        "print(f\"ðŸŽ¯ Overall Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output:\n",
        "```shell\n",
        "âœ… Tested on 100 random images\n",
        "ðŸ§¾ Total True: 3088, Predicted: 2504\n",
        "ðŸ“‰ Mean Absolute Error per image: 6.12\n",
        "ðŸŽ¯ Overall Accuracy: 0.81\n",
        "```"
      ],
      "metadata": {
        "id": "QN4P3Ax_j147"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}